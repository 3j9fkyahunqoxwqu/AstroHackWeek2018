{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Inference Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we'll cover:\n",
    "\n",
    "    1) Probability Distributions.\n",
    "    2) The sum and product rules of probability theory.\n",
    "    3) The interpretation of likelihoods, posteriors and priors.\n",
    "    4) An analytic implementation of simple Bayesian inference.\n",
    "    5) Hierarchical Bayesian models.\n",
    "    6) Probabilistic graphical models.\n",
    "    7) A numerical implementation of a hierarchical Bayesian model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies: you'll need to make sure you've got numpy and matplotlib installed in order to run this notebook. To install daft, type 'pip install daft' in your command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 reasons to use Bayesian inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    1) You want to infer the properties of some physical process by fitting a model to data.\n",
    "    2) When fitting a model to data you want to be explicit about your assumptions.\n",
    "    3) You'd like to produce probability distributions over model parameters.\n",
    "    4) Your problem is slightly more complicated that the simplest case.\n",
    "    5) E.g. you have a hierarchical model.\n",
    "    6) You'd like to compare alternative models.\n",
    "    7) You're working at the edge of observability.\n",
    "    8) Frequentist statistics are Hard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probability is the chance of a certain event x occurring. The frequentist way to measure a probability is to ask how many times event x occurred out of a number of trials. The probability is then the number of times the outcome was x divided by the number of trials.\n",
    "\n",
    "For discrete variables this is simple: e.g. to calculate the probability of a coin landing heads-up you would divide the number of time you got heads by the total number of coin tosses.\n",
    "\n",
    "But once you start to consider continuous variables: e.g. what is the probability that a person weighs between 170 and 173cm, you start having to perform integrals.\n",
    "\n",
    "Inference is all about solving integrals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Breads & spreads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joint, marginal and conditional probabilities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🥑, 🍞, 🥛🧀, 🍩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    " |   |🍩| 🍞| \n",
    " |---|---|---|\n",
    " | 🥛🧀| 5 | 1 |\n",
    " | 🥑| 3 | 7 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __joint__ probability is the probability of more than one thing. For example, your friend goes out to buy two ingredients for breakfast sandwiches: bread, X and spread, Y from a shop that only sells toast, bagels, avocado and cream cheese. You forgot to mention to your friend that you _really_ like bagels and cream cheese. You're hoping they'll bring bagels and cream cheese. If the probability of your friend buying bagels is $p(X = 🍩)$ and the probability of them buying cream cheese is $p(Y = 🥛🧀)$, then the joint probability of your friend buying both bagels and cream cheese is $p(X = 🍩, Y = 🥛🧀)$. This is a joint probability. Both bagels _and_ cream cheese would be a win."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Marginal__ probabilities are the probabilities we talk about most often. It's the probability of a single outcome where you ignore other outcomes that are related. For example, you might say \"what's the chance of rain today?\". The chance of rain is a marginal probability since the rain probability distribution is related to the humidity probability distribution and the pressure probability distribution, etc.\n",
    "\n",
    "If you ask Siri for the chance of rain she will tell you \"there is a 40% chance of rain today\". She has got that information from a weather prediction algorithm that has inferred the probability distribution of rain, marginalizing over the probability distributions of clouds, wind, temperature, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the bagel example. Really you only want the bagels because you love cream cheese. To be honest, you'd probably eat cream cheese on anything. What you really care about is whether your friend brings you cream cheese and you're not interested in whether they bring bagels. \n",
    "\n",
    "The probability that your friend brings back cream cheese, regardless of whether they bring back bagels is the probability of cream cheese, marginalized over the probability distribution over possible bread (X) values. If you want to know the total probability of cream cheese being part of your near future you need to add together the probabilities of cream cheese if $X=🍩$ and the probability if $X=🍞$.\n",
    "\n",
    "If the probability of cream cheese doesn't depend on X, then Y is not conditionally dependent on X and the probability of cream cheese becomes, $p(Y = 🥛🧀|X) = p(Y = 🥛🧀)$.\n",
    "\n",
    "But the probability of cream cheese almost certainly does depend on the choice of bread, in which case you'll need to marginalize over the probability distribution of X to find the probability of cream cheese:\n",
    "\n",
    "For discrete variables this marginalization is $p(Y = 🥛🧀) = p(Y=🥛🧀|X=🍩) + p(Y=🥛🧀|X=🍞)$ or \n",
    "$ \\sum_{i=1}^L p(Y=🥛🧀|X_i)$, where L is the number of bread choices.\n",
    "\n",
    "For continuous variables this marginalization becomes an integral. $p(Y) = \\int p(Y, X)dX$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __conditional__ probability is one where the probability of one thing depends on another thing. For example, what is the chance of rain given that it is cloudy? Or what is the chance of cream cheese given bagels?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps just as your friend was leaving you remembered to shout out, \"bagels! Get bagels!\". So you know you're getting bagels. Now you want the conditional probability of $Y=🥛🧀$, given that $X=🍩$. There's still a chance your friend will get avocado instead of cream cheese. It would be CRAZY but it's possible.\n",
    "\n",
    "You want to know the probability $Y=🥛🧀$ given $X=🍩$, \n",
    "\n",
    "$$p(Y=🥛🧀|~X=🍩)$$\n",
    "\n",
    "This is the marginal probability of cream cheese given bagels, or the probability of cream cheese, conditioned on the bread being bagels, $X=🍩$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In data analysis it's useful to write these equations in terms of data and model parameters. Ultimately, when fitting a model to data you're asking the question \"Which model parameters best reproduce these data points?\". You want the probability distributions over your model parameters, conditioned on the data you observe.\n",
    "\n",
    "Say you want to fit a straight line to some data. Your model parameters are $m$ and $b$ and your data are $N$ $x$ and $y$ values: $X = \\{x_1, x_2, ..., x_N\\}$ and $Y = \\{y_1, y_2, ..., y_N\\}$.\n",
    "\n",
    "You want the probability distribution over model parameters m and b given data $X$ and $Y$.\n",
    "\n",
    "$$P(m, b | X, Y)~~~~~~~~~1.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It might be even more useful to end up with a probability distribution over each model parameter, $P(m|X, Y)$ and $P(b|X, Y)$. As we'll see shortly, to calculate these we'll need to integrate over the probability distribution in the other dimension, or 'marginalize'.\n",
    "\n",
    "Notice that this is a conditional probability distribution. It wouldn't be very useful to have the joint probability of the data and the model, $P(m, b, X, Y)$. Writing probabilities as conditionals allows us to draw useful information out of these problems and make inferences about the world. It's also a natural outcome of any physical process. Physics (m and b) produces data (X and Y). We observe the data and use statistics to infer the physics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the product rule we can write 1 as, \n",
    "\n",
    "$$P(m, b|X, Y) = \\frac{P(m, b, X, Y)}{P(X, Y)}~~~~~~2.$$\n",
    "\n",
    "The joint probability of the data and the model doesn't tell us very much about the model. It's much more useful to rewrite equation 2 with a second conditional probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes' rule is:\n",
    "\n",
    "$$P(m, b|X, Y) = \\frac{P(X, Y|m, b)~P(m)P(b)}{P(X, Y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it can be derived from the product rule of probability theory. Try to derive this equation now. Hint: the joint probability of $m, b, X, Y$, can be written in any order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write $\\theta$ for the model parameters instead of m and b and D for the data instead of X and Y to simplify things. Bayes rule is,\n",
    "\n",
    "$$P(\\theta|D) = \\frac{P(D|\\theta)P(\\theta)}{P(D)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these terms has a name and interpretation.\n",
    "\n",
    "\n",
    "$P(\\theta)$ is the prior probability distribution. It represents your prior belief about the probability distribution over model parameters, before you have any data. You can update this prior belief once you have data, to get a posterior probability distribution over model parameters.\n",
    "\n",
    "$P(\\theta|D)$ is the posterior probability distribution. It's the probability distribution over model parameters, given the data you observe.\n",
    "\n",
    "$P(D|\\theta)$ is the likelihood. It's the probability distribution over the data you observe, given the model parameters. In other words it's the probability that you obtained the data set you have, given the model.\n",
    "\n",
    "$P(D)$ is the probability distribution over the data. It's the probability that you observed that data set. It's also sometimes called the evidence or the fully marginalized likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The likelihood explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does \"the probability of your data, given the model\" actually mean?\n",
    "\n",
    "Well, say you're observing a physical process that generates data with a Gaussian probability distribution, i.e. the probability of observing a certain value is a Gaussian centered on the \"truth\" with a standard deviation equal to its uncertainty. If there were a large number of outliers this might not be a good assumption. We'll also assume that the data are independent and identically distributed (iid). This means that the probability distribution of each data point doesn't depend on any other data point and that the probability distribution of each data point is identical. These assumptions are implicitly being made whenever a Gaussian likelihood function is used.\n",
    "\n",
    "The likelihood gives the probability that you observed the data you have, given the model parameters m and c. A Gaussian likelihood function for a single data point looks like this,\n",
    "\n",
    "$$\\mathcal{L_i} = p(y_i|m, c, x_i, \\sigma_y) = \\frac{1}{\\sigma_y\\sqrt{2\\pi}}\\exp\\left(\\frac{-[(mx_i + c) - x_i]^2}{2\\sigma_y^2}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: write a function for a Gaussian distribution as a python lambda function. This will come in useful throughout the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gauss = lambda mu, sigma, x: 1./(sigma * np.sqrt(2*np.pi)) * np.exp(-.5 * (mu - x) **2 / sigma**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The evidence explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evidence is the probability of the data. That's it. The overall probability of observing the data. It's the total probability of the data no matter what the model parameters are: independent of them. To calculate this you must sum the total probability of the data up over every possible model parameter value, i.e. marginalize over all model parameters.\n",
    "\n",
    "$p(D)$ can also be written, \n",
    "$$\\int p(D|\\theta)p(\\theta)d\\theta$$\n",
    "\n",
    "In the line fitting example this would be, \n",
    "$$p(Y) = \\int p(Y|m, c)p(m)p(c)dm dc$$\n",
    "\n",
    "The evidence or fully marginalized likelihood is the normalization of the posterior PDF. If you don't calculate this you can't know the overall probability of the model given the data, you can only know the relative probability of one set of model parameters vs another. This is often all we need to know for most inference problems. The evidence/FML is useful if you want to compare one model with another, e.g. a straight line vs a 2nd order polynomial (although there is a simpler way to do this than calculating the evidence/FML -- ask me about nested models).\n",
    "\n",
    "For a given model choice, the FML gives you the probability of seeing these data under your choice of model. If the evidence is low you know that this model does not describe the data very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes rule example: playing cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example from [Tiny Heero - Bayes Rule](http://tinyheero.github.io/2016/04/21/bayes-rule.html)\n",
    " who got it from [Brilliant - Bayes' Theorem and Conditional Probability](https://brilliant.org/wiki/bayes-theorem/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say you're playing cards and you want to know your chance of drawing a king from a freshly shuffled deck. You know a priori that the chance of drawing a king is $\\frac{1}{13}$.\n",
    "\n",
    "Now you are given a piece of information (data) that the top card is a face card. What is the chance that that card is a king now? What is your updated, posterior belief about the value of that card given the data -- that it's a face card.\n",
    "\n",
    "We can answer this question using Bayes' rule.\n",
    "\n",
    "$$P(\\theta = \\mathrm{King}~|~D = \\mathrm{face~card}) = \\frac{P(D = \\mathrm{face~card}~|~\\theta = \\mathrm{King})~P(\\theta = \\mathrm{King})}{P(D=\\mathrm{face~card})} $$\n",
    "\n",
    "We know the likelihood that the card is a face card given that it's a King: it must be 1. The probability of drawing a King is one in thirteen and the probability that the card is a face card is $\\frac{12}{52}$ = $\\frac{3}{13}$.\n",
    "\n",
    "So the updated, posterior probability that you draw a king given that you know the card is a face card is,\n",
    "\n",
    "$$P(\\theta = \\mathrm{King}~|~D = \\mathrm{face~card}) = \\frac{1 \\times \\frac{1}{13}}{\\frac{3}{13}} = \\frac{1}{3}$$\n",
    "\n",
    "A frequentist might answer this question by repeated trials. They would divide the number of times the card was a King  and also a face card by the number of times the card was a face card. After infinite trials they would find the probability to be exactly $\\frac{1}{3}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting a line to data.\n",
    "\n",
    "Here are some data generated by some deterministic process. There is a model that relates X to Y and Y is fully determined by X and some noise property, $\\sigma_Y$.\n",
    "\n",
    "Draw a probabilistic graphical model for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'y')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIwgAIHMEAQBk7n8BU2t7hVhjSkQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1063b9fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 20\n",
    "op = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "x = np.random.uniform(0, 10, size=N)\n",
    "sigma = .3  # Homoskedastic uncertainties, all uncerts take this value.\n",
    "yerr = np.ones_like(x) * sigma\n",
    "\n",
    "# make up a model\n",
    "m, c = .8, 3\n",
    "y = m * x + c + np.random.randn(N) * sigma\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".\")\n",
    "plt.errorbar(x[op], y[op], yerr[op], fmt=\".\")  # highlight one data point in orange\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write Bayes rule as, \n",
    "\n",
    "$$p(m, c| \\{y_i\\}_{i=1}^N, \\{x_i\\}_{i=1}^N, \\sigma_y ) \\propto p(\\{y_i\\}_{i=1}^N | m, c, \\{x_i\\}_{i=1}^N, \\sigma_y) p(m)p(c)$$\n",
    "\n",
    "To make this more readable, we'll drop the $x$ and $\\sigma$,\n",
    "\n",
    "$$p(m, c| \\{y_i\\}_{i=1}^N) \\propto p(\\{y_i\\}_{i=1}^N | m, c) p(m)p(c)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the probability of observing a certain y-value is a Gaussian centered on the \"truth\" with a standard deviation equal to its uncertainty, then we can use a Gaussian likelihood function for this inference problem. If there were a large number of outliers (i.e. the data were not Gaussian distributed) this might not be a good assumption. We'll also assume that the data are independent and identically distributed (iid). This means that the probability distribution of each data point doesn't depend on any other data point and that the probability distribution of each data point is identical. These assumptions are implicitly being made whenever a Gaussian likelihood function is used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the likelihood of that orange data point? As in, what's the likelihood of observing that data point, given the model, y = mx + c, and the chosen x value? The coordinates of that data point are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x[op], y[op])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The likelihood is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{L} = p(y_2|m, c, x_2, \\sigma_y) = \\frac{1}{(2\\pi\\sigma_y^2)^{N/2}}\\exp\\left[-\\frac{1}{2\\sigma_y^2}(y_2 - \\mu)^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here $\\mu$ is the mean. We're used to thinking of the mean as a single number. But here we're saying that each data point is drawn from a Gaussian distribution __centered on the straight line__. The straight line is the mean and since it's a model instead of a number we'll call it the mean model. So $\\mu = mx + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ = \\frac{1}{(2\\pi\\sigma_y^2)^{N/2}}\\exp\\left[-\\frac{1}{2\\sigma_y^2}(y_2 - [mx_2 + c])^2 \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plug in the numbers to calculate the likelihood of that data point. Define a variable equal to that likelihood and call the variable yop_likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Why can this value be greater than 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr, fmt=\".\")\n",
    "plt.errorbar(x[op], y[op], yerr[op], fmt=\".\")  # highlight one data point in orange\n",
    "\n",
    "xplot = np.linspace(0, 10, 100)\n",
    "plt.plot(xplot, m*xplot + c)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(x[op]-2, x[op]+2)\n",
    "plt.ylim(y[op]-2, y[op]+2)\n",
    "\n",
    "yplot = np.linspace(y[op]-1, y[op]+1.5, 100)\n",
    "plt.plot((Gauss((m*x[op] + c), sigma, yplot))*.5 + x[op], yplot, \".5\", ls=\"--\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability distribution over y-values, given the model and the x value is shown by the grey, dashed Gaussian shape. Here it is on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(yplot, Gauss((m*x[op] + c), sigma, yplot))\n",
    "plt.ylabel(\"likelihood of data point 2\")\n",
    "plt.xlabel(\"y\")\n",
    "plt.axvline(y[op], color=\"tab:orange\", ls=\"--\")\n",
    "plt.axhline(yop_likelihood, color=\"tab:orange\", ls=\"--\");\n",
    "print(y[op], yop_likelihood)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The orange line is the observation. This line intersects the curve at the likelihood calculated above. What happens if the values of the model parameters change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr, fmt=\".\")\n",
    "plt.errorbar(x[op], y[op], yerr[op], fmt=\".\")  # highlight one data point in orange\n",
    "\n",
    "# Plot lines with different slopes\n",
    "ms = np.linspace(m, m-.1, 5)\n",
    "for mi in ms:\n",
    "    plt.plot(xplot, mi*xplot + c, color=\"tab:green\", alpha=.5, ls=\"--\")\n",
    "    \n",
    "plt.plot(xplot, m*xplot + c, color=\"tab:green\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.xlim(x[op]-2, x[op]+2)\n",
    "plt.ylim(y[op]-2, y[op]+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the likelihood of the orange data point for models with different slopes\n",
    "for mi in ms:\n",
    "    plt.plot(yplot, Gauss((mi*x[op] + c), sigma, yplot), alpha=.5, color=\"tab:blue\")\n",
    "\n",
    "plt.plot(yplot, Gauss((m*x[op] + c), sigma, yplot), color=\"tab:blue\")\n",
    "plt.ylabel(\"likelihood of data point 3\")\n",
    "plt.xlabel(\"y\")\n",
    "plt.axvline(y[op], color=\"tab:orange\", ls=\"--\")\n",
    "plt.axhline(yop_likelihood, color=\"tab:orange\", ls=\"--\")\n",
    "\n",
    "for mi in ms:\n",
    "    plt.axhline(Gauss((mi*x[op] + c), sigma, y[op]), color=\"tab:orange\", ls=\"--\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the likelihood of this data point as a function of various values for the slope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mgrid = np.linspace(0, 1.5, 1000)\n",
    "m_dist = Gauss((mgrid*x[op] + c), sigma, y[op])\n",
    "\n",
    "plt.plot(mgrid, m_dist)\n",
    "plt.axvline(m, color=\"tab:orange\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why isn't the maximum of this distribution 0.8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "for ms in mgrid[::5]:\n",
    "    plt.plot(xplot, ms*xplot + c, alpha=.2)\n",
    "plt.plot(xplot, m*xplot + c, \"tab:green\")\n",
    "plt.errorbar(x[op], y[op], yerr=yerr[3], fmt=\".\", color=\"tab:orange\")\n",
    "plt.xlim(x[op]-2, x[op]+2)\n",
    "plt.ylim(y[op]-2, y[op]+2)\n",
    "\n",
    "max_m = mgrid[m_dist == max(m_dist)]\n",
    "print(max_m)\n",
    "plt.plot(xplot, max_m*xplot + c, \"tab:blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How about the likelihood of all the data, given the model? This is just the product of likelihoods for the individual data points.\n",
    "\n",
    "$$\\mathcal{L} = p(\\{y_i\\}_{i=1}^N|m, c) = \\prod_{i=1}^Np(y_i|m, c)$$\n",
    "\n",
    "Calculate the total likelihood of all the data in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise this number can get very small so it is usually more convenient to work with the log likelihood, \n",
    "\n",
    "$$\\ln(\\mathcal{L}) = \\ln\\left[p(\\{y_i\\}_{i=1}^N|m, c)\\right] = \\sum_{i=1}^N \\ln \\left[ p(y_i|m, c)\\right]  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the log likelihood in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the likelihood of all the data, given a range of model parameters, m and c.\n",
    "\n",
    "Use the cell below to define a log-likelihood function (not as a lambda function, just a regular function). This function should take the list of parameters [m, c], x, y and yerr as arguments and return the total log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrid_m = 100\n",
    "ngrid_c = 200\n",
    "mmin, mmax, cmin, cmax = .6, 1, 2, 4\n",
    "_M, _C = np.linspace(mmin, mmax, ngrid_m), np.linspace(cmin, cmax, ngrid_c)\n",
    "M, C = np.meshgrid(_M, _C, indexing=\"ij\")\n",
    "\n",
    "lnlikes = np.zeros((ngrid_m, ngrid_c))\n",
    "for i in range(ngrid_m):\n",
    "    for j in range(ngrid_c):\n",
    "        par = [M[i, j], C[i, j]]\n",
    "        lnlikes[i, j] = lnlike(par, x, y, yerr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the resulting 2d distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(_M, _C, np.exp(lnlikes.T))\n",
    "plt.colorbar(label=\"likelihood\")\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the marginals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.xlabel(\"m\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.xlabel(\"c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the prior?\n",
    "\n",
    "Reminder: Bayes rule is.\n",
    "\n",
    "$$p(\\theta|D) = \\frac{p(D|\\theta) p(\\theta)}{p(D)}$$\n",
    "\n",
    "$$p(m, c|\\{y_i\\}_{i=1}^N) \\propto p(\\{y_i\\}_{i=1}^N|m, c) p(\\theta)$$\n",
    "\n",
    "$$ \\mathrm{posterior} \\propto \\mathrm{likelihood} \\times \\mathrm{prior}$$\n",
    "\n",
    "So, in order to calculate the posterior we need to multiply the likelihood by a prior. We're using a Gaussian likelihood function and if we also use a Gaussian prior, we can calculate the posterior analytically because a Gaussian multiplied by a Gaussian is another Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some broad priors over the model parameters. Complete the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(par):\n",
    "    m, c = par\n",
    "    \n",
    "    # A Gaussian prior over m, with standard deviation = 0.5, and mean = 1\n",
    "    \n",
    "    # A Gaussian prior over c, with standard deviation = 1 and mean = 3\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluate the prior everywhere we evaluated the likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priors = np.zeros((ngrid_m, ngrid_c))\n",
    "for i in range(ngrid_m):\n",
    "    for j in range(ngrid_c):\n",
    "        par = [M[i, j], C[i, j]]\n",
    "        priors[i, j] = prior(par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 2D distribution and the marginals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(_M, _C, priors.T)\n",
    "plt.colorbar(label=\"prior\")\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"c\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(M[:, 0], np.sum(priors, axis=1));\n",
    "plt.xlabel(\"m\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C[0, :], np.sum(priors, axis=0));\n",
    "plt.xlabel(\"c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now calculating the posterior is as easy as multiplying the likelihood by the prior. Go ahead!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pcolor(_M, _C, posterior.T)\n",
    "plt.colorbar(label=\"posterior\")\n",
    "plt.xlabel(\"m\")\n",
    "plt.ylabel(\"c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the marginals again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(M[:, 0], np.sum(posterior, axis=1));\n",
    "plt.xlabel(\"m\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(C[0, :], np.sum(posterior, axis=0));\n",
    "plt.xlabel(\"c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though my prior over m was a little bit wrong, it didn't matter because the data strongly support the correct model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This posterior is proportional to a new Gaussian:\n",
    "\n",
    "$$p(m, c, \\{x_i\\}_{i=1}^N, \\sigma_y|\\{y_i\\}_{i=1}^N) \\propto \\frac{1}{\\sigma_y\\sqrt{2\\pi}} \\prod_{i=1}^N \\exp\\left(-\\frac{[(m\\{x_i\\}_{i=1}^N + c) - \\{y_i\\}_{i=1}^N]^2}{2\\sigma_y^2}\\right) \\frac{1}{\\sigma_m\\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\mu_m - m)^2}{2\\sigma_m^2} \\right) \\frac{1}{\\sigma_c\\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\mu_c - c)^2}{2\\sigma_c^2} \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and if we took the derivative of this Gaussian with respect to the model parameters we'd be able to calculate the normalised posterior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(m, c|\\{y_i\\}_{i=1}^N) = \\frac{\\frac{1}{\\sigma_y\\sqrt{2\\pi}} \\prod_{i=1}^N \\exp\\left(-\\frac{[(m\\{x_i\\}_{i=1}^N + c) - \\{y_i\\}_{i=1}^N]^2}{2\\sigma_y^2}\\right) \\frac{1}{\\sigma_m\\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\mu_m - m)^2}{2\\sigma_m^2} \\right) \\frac{1}{\\sigma_c\\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\mu_c - c)^2}{2\\sigma_c^2} \\right)}  {\\int_{-\\infty}^\\infty \\frac{1}{\\sigma_y\\sqrt{2\\pi}} \\prod_{i=1}^N \\exp\\left(-\\frac{[(m\\{x_i\\}_{i=1}^N + c) - \\{y_i\\}_{i=1}^N]^2}{2\\sigma_y^2}\\right) \\frac{1}{\\sigma_m\\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\mu_m - m)^2}{2\\sigma_m^2} \\right) \\frac{1}{\\sigma_c\\sqrt{2\\pi}} \\exp\\left( - \\frac{(\\mu_c - c)^2}{2\\sigma_c^2} \\right) dm dc}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the likelihood and priors we used were Gaussian, the posterior is also a Gaussian and the evidence is an integral over a Gaussian which is analytic. It is therefore possible to calculate the posterior analytically, but that's above my pay-grade for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the likelihoods of each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 9))\n",
    "yp = np.linspace(0, 15, 1000)\n",
    "\n",
    "for xi in x: \n",
    "    plt.plot(yp, Gauss((m*xi + c), sigma, yp))\n",
    "\n",
    "plt.ylabel(\"likelihood of observing data point given model\")\n",
    "plt.xlabel(\"y\")\n",
    "plt.xlim(2, 12)\n",
    "plt.ylim(0, 1.5)\n",
    "\n",
    "for i, yi in enumerate(y):\n",
    "    plt.axvline(yi, ls=\"--\", alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why are these all the same height and width?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frequentism vs Bayesianism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequentists see probabilities as the outcomes of repeated trials, where the probability of an outcome is its frequency divided by the total number of trials. The more trials, the closer this probability gets to the true probability. In Bayesian statistics, prior beliefs about the probability of an event are incorporated into the calculation.\n",
    "\n",
    "One of the biggest practical differences are that Bayesians are explicit about the priors they use. In a sense frequentists still have priors they're just 'improper' priors. \n",
    "\n",
    "The other main difference is that it isn't very meaningful to think about the probability of a single event if you're a frequentist. Frequentist probabilities represent the frequency of a certain event occurring, given a number of trials. Bayesians see the world probabilistically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a line to data: a fully worked example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "\n",
    "x = np.random.uniform(0, 10, size=N)\n",
    "y_sigma = 3  # Homoskedastic uncertainties, all uncerts take this value.\n",
    "yerr = np.ones_like(x) * y_sigma\n",
    "\n",
    "# make up a model\n",
    "true_m, true_c = 4, 15\n",
    "y = true_m * x + true_c + np.random.randn(N) * y_sigma\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first use linear least squares fitting. This model is linear and the maximum likelihood values of the model parameters m and c can be found analytically using linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = mx + c$$\n",
    "\n",
    "$$[c, m] = (A^T C^{-1} A)^{-1} A^T C^{-1} y$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AT = np.vstack((np.ones_like(x), x))\n",
    "C = np.eye(N) * y_sigma**2\n",
    "\n",
    "ATC_invA = np.dot(AT, np.linalg.solve(C, AT.T))\n",
    "ATC_invy = np.dot(AT, np.linalg.solve(C, y))\n",
    "freq_c, freq_m = np.linalg.solve(ATC_invA, ATC_invy)\n",
    "\n",
    "c_err, m_err = np.sqrt(np.diag(np.linalg.inv(ATC_invA)))\n",
    "\n",
    "print(\"c = {0:.2} +/- {1:.2}, m = {2:.2} +/- {3:.2}\".format(freq_c, c_err, freq_m, m_err))\n",
    "print(\"true c = \", true_c, \"true m = \", true_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.errorbar(x, y, yerr=yerr, fmt=\".\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "xplot = np.linspace(0, 10, 100)\n",
    "plt.plot(xplot, freq_m*xplot + freq_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if you wanted to use a prior that doesn't bias your results toward higher slopes you could do MCMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnlike(par, args):\n",
    "    x, y, y_sigma = args\n",
    "    c, m = par\n",
    "    mean_model = m*x + c\n",
    "    return -.5 * sum((y - mean_model)**2 / y_sigma**2)\n",
    "\n",
    "def lnprior(par):\n",
    "    \"\"\"\n",
    "    From Jake Vanderplas's blog post: \n",
    "    https://jakevdp.github.io/blog/2014/06/14/frequentism-and-bayesianism-4-bayesian-in-python/\n",
    "    \"\"\"\n",
    "    c, m = par\n",
    "    return -1.5 * np.log(1 + m ** 2)\n",
    "\n",
    "def uniform_lnprior(par):\n",
    "    c, m = par\n",
    "    if -10 < c < 10 and -10 < m < 10:\n",
    "        return 0\n",
    "    else:\n",
    "        return -np.inf\n",
    "\n",
    "def lnprob(par, *args):\n",
    "    x, y, y_sigma = args\n",
    "    return lnlike(par, args) + lnprior(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "ndim, nwalkers = 2, 50\n",
    "p_init = 1, 1\n",
    "\n",
    "args = [x, y, y_sigma]\n",
    "p0 = [np.random.rand(ndim)*1e-4 + p_init for i in range(nwalkers)]\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob, args=args)\n",
    "p0, _, _ = sampler.run_mcmc(p0, 1000);\n",
    "sampler.reset()\n",
    "sampler.run_mcmc(p0, 1000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "corner.corner(sampler.flatchain, labels=[\"c\", \"m\"], truths=[true_c, true_m]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_samps, m_samps = sampler.flatchain.T\n",
    "med_c, med_m = np.median(c_samps), np.median(m_samps)\n",
    "c_err, m_err = np.std(c_samps), np.std(m_samps)\n",
    "c_errm, c_errp = med_c - np.percentile(c_samps, 16), np.percentile(c_samps, 84) - med_c\n",
    "m_errm, m_errp = med_m - np.percentile(m_samps, 16), np.percentile(m_samps, 84) - med_m\n",
    "\n",
    "print(\"c = {0:.2} +/- {1:.2}\".format(med_c, c_err), \"m = {0:.2} +/- {1:.2}\".format(med_m, m_err))\n",
    "print(\"true c = \", true_c, \"true m = \", true_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
